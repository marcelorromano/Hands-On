{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando Redes Profundas\n",
    "- Consultar Tabelas 11-3 e 11-4 para sugestões de redes que podem funcionar na prática.\n",
    "- O exercício no fim do capítulo aparentemente indicou que SELU com dropout não é tão bom como Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T15:45:29.616052Z",
     "start_time": "2021-10-06T15:45:27.978846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Problema de Gradientes Explosivos ou que se Anulam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Inicialização de Xavier/Glorot ou de He\n",
    "- Essas inicializações buscam garantir que a variância (distribuição normal ou uniforme) da entrada é igual à da saída, de forma que os gradientes não se anulem.\n",
    "- Tomam como base o fan<sub>in</sub> e o fan<sub>out</sub>\n",
    "- Se recomenda, para distribuições normais, as seguintes inicializações: Glorot, para tanh ou logística, He, para ReLU e LeCun para a SELU.\n",
    "- Por padrão o Keras usa a inicialização de Glorot com distribuição uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T15:03:38.074974Z",
     "start_time": "2021-10-07T15:03:31.460371Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x2557481a1c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:11:30.262520Z",
     "start_time": "2021-10-01T14:11:30.246530Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1bb4af28040>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Funções de Ativação que não Saturam\n",
    "- ReLU, <span style='font-style:italic;'>leaky</span> ReLU, <span style='font-style:italic;'>randomized leaky</span> ReLU (RReLU), <span style='font-style:italic;'>parametric leaky</span> ReLU (PReLU: o &alpha; é aprendido durante o treinamento), ELU (para valores negativos há a suavização por uma exponencial) e SELU (versão escalonada da ELU que resolve o problema de gradientes que se anulam se os dados de entrada tiverem distribuição normal padrão, se não houver atalhos na rede se e as demais camadas seguirem a inicialização <span style='font-family:monospace;'>lecun_normal</span>)\n",
    "- Em geral: SELU (não é suavizada como a ELU) > ELU > <span style='font-style:italic;'>leaky</span> ReLU > RELU > <span style='font-style:italic;'>tanh</span>() > logística. Se a latência for muito importante, RELU pode ser mais adequada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:29:38.819286Z",
     "start_time": "2021-10-01T14:29:38.621732Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Batch Normalization\n",
    "- O parâmetro <span style='font-family:monospace'>axis</span> se comporta de forma contrária à biblioteca numpy, em que o axis indica a dimensão que será colapsada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T15:02:26.084931Z",
     "start_time": "2021-10-01T15:02:25.974359Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False), # O Bias não é necessário antes da BatchNormalization()\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Gradient Clipping\n",
    "- Restringir o valor de uma das componentes ou a norma do gradiente. É mais necessário nas redes recorrentes, nas quais Batch Normalization não é tão efetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T12:42:40.664184Z",
     "start_time": "2021-10-04T12:42:40.649206Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)\n",
    "# model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Reusando Camadas Pré-Treinadas\n",
    "- Quando tarefas são similares, pode-se utilizar redes já treinadas e substituir as últimas camadas. Isso faz com que não sejam necessárias tantas amostras para o treinamento. Ex: classificação de imagens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Transfer Learning com Keras\n",
    "- Fashion MNIST: X_train_A &#8594; todas as imagens, exceto as sandálias e camisetas (classes 5 e 6). X_train_B &#8594; apenas 200 imagens de sandálias e camisetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T13:28:49.938567Z",
     "start_time": "2021-10-04T13:28:49.922576Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6)\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # os índices das classes 7,8,9 devem ser movidos para 5,6,7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # classificação binária\n",
    "    return ((X[~y_5_or_6], y_A), (X[y_5_or_6], y_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:06:16.246594Z",
     "start_time": "2021-10-04T14:06:15.506667Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print(X_train_full.shape)\n",
    "print(X_train_full.dtype)\n",
    "X_valid, X_train, X_test = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0, X_test / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:06:21.275833Z",
     "start_time": "2021-10-04T14:06:21.093586Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T13:31:18.543814Z",
     "start_time": "2021-10-04T13:31:18.525811Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43986, 28, 28)\n",
      "(200, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_A.shape)\n",
    "print(X_train_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T13:40:50.483540Z",
     "start_time": "2021-10-04T13:39:57.509426Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 4s 2ms/step - loss: 0.5926 - accuracy: 0.8103 - val_loss: 0.3890 - val_accuracy: 0.8677\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.3523 - accuracy: 0.8786 - val_loss: 0.3290 - val_accuracy: 0.8822\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.3170 - accuracy: 0.8895 - val_loss: 0.3014 - val_accuracy: 0.8989\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2972 - accuracy: 0.8974 - val_loss: 0.2892 - val_accuracy: 0.9016\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2834 - accuracy: 0.9023 - val_loss: 0.2774 - val_accuracy: 0.9071\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2729 - accuracy: 0.9063 - val_loss: 0.2733 - val_accuracy: 0.9071\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2640 - accuracy: 0.9091 - val_loss: 0.2719 - val_accuracy: 0.9088\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2572 - accuracy: 0.9125 - val_loss: 0.2589 - val_accuracy: 0.9141\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2518 - accuracy: 0.9136 - val_loss: 0.2562 - val_accuracy: 0.9143\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2468 - accuracy: 0.9155 - val_loss: 0.2542 - val_accuracy: 0.9155\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2422 - accuracy: 0.9177 - val_loss: 0.2496 - val_accuracy: 0.9155\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2382 - accuracy: 0.9187 - val_loss: 0.2511 - val_accuracy: 0.9128\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2350 - accuracy: 0.9197 - val_loss: 0.2446 - val_accuracy: 0.9158\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2315 - accuracy: 0.9214 - val_loss: 0.2413 - val_accuracy: 0.9175\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2287 - accuracy: 0.9214 - val_loss: 0.2447 - val_accuracy: 0.9195\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2253 - accuracy: 0.9226 - val_loss: 0.2386 - val_accuracy: 0.9193\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2230 - accuracy: 0.9232 - val_loss: 0.2403 - val_accuracy: 0.9178\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2200 - accuracy: 0.9244 - val_loss: 0.2425 - val_accuracy: 0.9155\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2177 - accuracy: 0.9254 - val_loss: 0.2331 - val_accuracy: 0.9203\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2155 - accuracy: 0.9262 - val_loss: 0.2332 - val_accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation='selu'))\n",
    "model_A.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "model_A.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20, validation_data=(X_valid_A, y_valid_A))\n",
    "\n",
    "model_A.save('my_model_A.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T13:41:52.152113Z",
     "start_time": "2021-10-04T13:41:49.163590Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 46ms/step - loss: 0.9573 - accuracy: 0.4650 - val_loss: 0.6314 - val_accuracy: 0.6004\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.5692 - accuracy: 0.7450 - val_loss: 0.4784 - val_accuracy: 0.8529\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.4503 - accuracy: 0.8650 - val_loss: 0.4102 - val_accuracy: 0.8945\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3879 - accuracy: 0.8950 - val_loss: 0.3647 - val_accuracy: 0.9178\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3435 - accuracy: 0.9250 - val_loss: 0.3300 - val_accuracy: 0.9320\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3081 - accuracy: 0.9300 - val_loss: 0.3019 - val_accuracy: 0.9402\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.2800 - accuracy: 0.9350 - val_loss: 0.2804 - val_accuracy: 0.9422\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.2564 - accuracy: 0.9450 - val_loss: 0.2606 - val_accuracy: 0.9473\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2362 - accuracy: 0.9550 - val_loss: 0.2428 - val_accuracy: 0.9523\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2188 - accuracy: 0.9600 - val_loss: 0.2281 - val_accuracy: 0.9544\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.2036 - accuracy: 0.9700 - val_loss: 0.2150 - val_accuracy: 0.9584\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1898 - accuracy: 0.9700 - val_loss: 0.2036 - val_accuracy: 0.9584\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1773 - accuracy: 0.9750 - val_loss: 0.1931 - val_accuracy: 0.9615\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1668 - accuracy: 0.9800 - val_loss: 0.1838 - val_accuracy: 0.9635\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1570 - accuracy: 0.9900 - val_loss: 0.1746 - val_accuracy: 0.9686\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1481 - accuracy: 0.9900 - val_loss: 0.1674 - val_accuracy: 0.9686\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1406 - accuracy: 0.9900 - val_loss: 0.1604 - val_accuracy: 0.9706\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1334 - accuracy: 0.9900 - val_loss: 0.1539 - val_accuracy: 0.9706\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1268 - accuracy: 0.9900 - val_loss: 0.1482 - val_accuracy: 0.9716\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1208 - accuracy: 0.9900 - val_loss: 0.1431 - val_accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T13:58:58.504114Z",
     "start_time": "2021-10-04T13:58:58.379465Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                2550      \n",
      "=================================================================\n",
      "Total params: 275,750\n",
      "Trainable params: 275,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model_A = keras.models.load_model('my_model_A.h5')\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.summary()\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T13:59:16.948647Z",
     "start_time": "2021-10-04T13:59:16.871283Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 408       \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_A_clone.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:00:18.141697Z",
     "start_time": "2021-10-04T14:00:17.275291Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 37ms/step - loss: 0.4515 - accuracy: 0.8350 - val_loss: 0.4461 - val_accuracy: 0.8174\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.4188 - accuracy: 0.8500 - val_loss: 0.4167 - val_accuracy: 0.8306\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.3899 - accuracy: 0.8600 - val_loss: 0.3900 - val_accuracy: 0.8448\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.3636 - accuracy: 0.8700 - val_loss: 0.3668 - val_accuracy: 0.8540\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:00:52.232603Z",
     "start_time": "2021-10-04T14:00:50.422867Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 32ms/step - loss: 0.2965 - accuracy: 0.9250 - val_loss: 0.2504 - val_accuracy: 0.9523\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2079 - accuracy: 0.9850 - val_loss: 0.1981 - val_accuracy: 0.9807\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1648 - accuracy: 0.9900 - val_loss: 0.1665 - val_accuracy: 0.9899\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1372 - accuracy: 1.0000 - val_loss: 0.1461 - val_accuracy: 0.9919\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1192 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9929\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1053 - accuracy: 1.0000 - val_loss: 0.1194 - val_accuracy: 0.9929\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0950 - accuracy: 1.0000 - val_loss: 0.1103 - val_accuracy: 0.9919\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0865 - accuracy: 1.0000 - val_loss: 0.1025 - val_accuracy: 0.9919\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0792 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9919\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0732 - accuracy: 1.0000 - val_loss: 0.0904 - val_accuracy: 0.9919\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0681 - accuracy: 1.0000 - val_loss: 0.0856 - val_accuracy: 0.9919\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.0813 - val_accuracy: 0.9919\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0596 - accuracy: 1.0000 - val_loss: 0.0774 - val_accuracy: 0.9919\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0560 - accuracy: 1.0000 - val_loss: 0.0742 - val_accuracy: 0.9919\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9919\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0503 - accuracy: 1.0000 - val_loss: 0.0687 - val_accuracy: 0.9919\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:06:29.471954Z",
     "start_time": "2021-10-04T14:06:29.206138Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9705\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.9980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06370459496974945, 0.9980000257492065]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)\n",
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:06:55.525473Z",
     "start_time": "2021-10-04T14:06:55.514479Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.749999999999805"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 - 97.05) / (100 - 99.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Comentários\n",
    "- A taxa de erro caiu quase 15 vezes, entretanto, transfer learning não funciona tão bem para redes pequenas fully-connected, que detectam padrões mais específicos. Elas são mais úteis em redes profundas convolucionais, pois elas são capazes de detectar padrões mais gerais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Pré-Treinamento Não-Supervisionado\n",
    "- Quando há poucos dados rotulados e muitos não rotulados, pode-se treinar uma GAN ou um autoencoder com os dados não rotulados.\n",
    "- Feito isso, se utiliza o autoencoder ou o discriminador da GAN e se adiciona outras camadas para se fazer o treinamento supervisionado com os dados rotulados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Pré-Treinamento em uma Tarefa Auxiliar\n",
    "- Quando há poucos dados para a tarefa específica, mas muitos dados para outras tarefas similares. Nesse caso, pode-se treinar o modelo para a tarefa similar e depois re-treinar as últimas camadas para a tarefa específica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Otimizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Termo de Momento\n",
    "- O gradiente se torna termo de aceleração, não velocidade:\n",
    "<br>\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>m</strong> &larr; &beta;<strong>m</strong> - &eta;&nabla;<sub style='font-weight:bold;'>&theta;</sub>J(<strong>&theta;</strong>)</p>\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>&theta;</strong> = <strong>&theta;</strong> + <strong>m</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T13:44:47.374566Z",
     "start_time": "2021-10-05T13:44:47.354740Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9) # lr = η e momentum = β "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Gradiente Acelerado de Nesterov (NAG)\n",
    "- Se calcula o gradiente no local onde o vetor de pesos está indo, acelerando a convergência.\n",
    "- Geralmente o NAG se comporta melhor e é mais rápido que o método padrão\n",
    "<br>\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>m</strong> &larr; &beta;<strong>m</strong> - &eta;&nabla;<sub style='font-weight:bold;'>&theta;</sub>J(<strong>&theta;</strong>+&beta;<strong>m</strong>)</p>\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>&theta;</strong> = <strong>&theta;</strong> + <strong>m</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T13:53:09.938218Z",
     "start_time": "2021-10-05T13:53:09.929207Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Adagrad\n",
    "- Penaliza as componentes dos gradientes que já foram muito ajustadas, fazendo com que a direção de ajuste permaneça mais constante.\n",
    "- Entretanto, não é recomendado para redes profundas, pois acaba zerando as componentes muito rápido\n",
    "<br>\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>s</strong> &larr; <strong>s</strong> + &nabla;<sub style='font-weight:bold;'>&theta;</sub><sup>2</sup>J(<strong>&theta;</strong>)</p>\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>&theta;</strong> = <strong>&theta;</strong> - &eta;&nabla;<sub style='font-weight:bold;'>&theta;</sub>J(<strong>&theta;</strong>) / <span style=\"white-space: nowrap;\">&radic;<span style=\"text-decoration:overline;\">&nbsp;<strong>s</strong> + &epsilon;&nbsp;</span>\n",
    "</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T14:53:05.145244Z",
     "start_time": "2021-10-05T14:53:05.140248Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Adadelta\n",
    "- Conserta o algoritmo anterior, adicionando o termo de esquecimento de forma que o <strong>s</strong> não cresça de forma exagerada.\n",
    "<br>\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>s</strong> &larr; &beta;<strong>s</strong> + (1 - &beta;)&nabla;<sub style='font-weight:bold;'>&theta;</sub><sup>2</sup>J(<strong>&theta;</strong>)</p>\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>&theta;</strong> = <strong>&theta;</strong> - &eta;&nabla;<sub style='font-weight:bold;'>&theta;</sub>J(<strong>&theta;</strong>) / <span style=\"white-space: nowrap;\">&radic;<span style=\"text-decoration:overline;\">&nbsp;<strong>s</strong> + &epsilon;&nbsp;</span>\n",
    "</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T14:52:05.555495Z",
     "start_time": "2021-10-05T14:52:05.539520Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9) # lr = η e rho = β "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Adam\n",
    "- Combina o Adadelta com o termo de momento, mas fazendo média exponencial decadente em vez de soma exponencial decadente (a diferença é apenas de uma constante multiplicativa).\n",
    "- Os termos adicionais corrigem o viés em torno de zero para as primeiras iterações.\n",
    "\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>m</strong> &larr; &beta;<sub>1</sub><strong>m</strong> - (1 - &beta;<sub>1</sub>)&nabla;<sub style='font-weight:bold;'>&theta;</sub>J(<strong>&theta;</strong>)</p>\n",
    "\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>s</strong> &larr; &beta;<sub>2</sub><strong>s</strong> + (1 - &beta;<sub>2</sub>)&nabla;<sub style='font-weight:bold;'>&theta;</sub><sup>2</sup>J(<strong>&theta;</strong>)</p>\n",
    "\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>m</strong> &larr; <strong>m</strong> /(1 - &beta;<sub>1</sub><sup>T</sup>)</p>\n",
    "\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>s</strong> &larr; <strong>s</strong> /(1 - &beta;<sub>2</sub><sup>T</sup>)</p>\n",
    "\n",
    "<p style='font-style:italic;font-size:2rem;text-align:center;'><strong>&theta;</strong> = <strong>&theta;</strong> - &eta;<strong>m</strong> / <span style=\"white-space: nowrap;\">&radic;<span style=\"text-decoration:overline;\">&nbsp;<strong>s</strong> + &epsilon;&nbsp;</span>\n",
    "</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T13:39:24.487085Z",
     "start_time": "2021-10-06T13:39:24.466097Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999) # o epsilon padrão do Keras é 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Programação da Taxa de Aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Power Scheduling\n",
    "- Tipicamente, c = 1. A taxa cai a cada s passos: &eta;<sub>0</sub>/2, &eta;<sub>0</sub>/3, etc.\n",
    "<br> \n",
    "<p style='font-style:italic;font-size:1.5em;text-align:center;'>&eta;(t) = &eta;<sub>0</sub> /(1 + t/s)<sup>c</sup></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:11:16.014200Z",
     "start_time": "2021-10-06T14:11:15.998213Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4) # decay é o inverso de s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Exponencial Scheduling\n",
    "- A taxa cai 10 vezes a cada s passos.\n",
    "<br> \n",
    "<p style='font-style:italic;font-size:1.5em;text-align:center;'>&eta;(t) = &eta;<sub>0</sub> 0.1<sup>t/s</sup></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:27:44.827608Z",
     "start_time": "2021-10-06T14:27:44.811616Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0*0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\n",
    "\n",
    "# Há a opção de fazer a cada batch de cada época, consultar o código do GitHub se um dia for necessário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Piecewise Constant Scheduling\n",
    "- Constante por partes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:31:19.910269Z",
     "start_time": "2021-10-06T14:31:19.894279Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Performance Scheduling\n",
    "- Mede o erro de validação a cada N passos e reduz a taxa por um fator &lambda; se o erro parar de cair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:36:09.664253Z",
     "start_time": "2021-10-06T14:36:09.646244Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1cycle Scheduling\n",
    "- Inicia com uma taxa &eta;<sub>0</sub>, sobe até &eta;<sub>1</sub> no meio do treinamento, volta até &eta;<sub>0</sub> no fim e, nas últimas épocas, diminui a taxa linearmente por várias ordens de magnitude. &eta;<sub>1</sub> seria a máxima taxa de aprendizado e &eta;<sub>0</sub> 10 vezes menor que &eta;<sub>1</sub>. O momento também seguiria uma abordagem parecida: 0,95/0,85/0,95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:51:23.645494Z",
     "start_time": "2021-10-06T14:51:23.566545Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)\n",
    "        \n",
    "# n_epochs = 25\n",
    "# onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.05)\n",
    "# history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "#                     validation_data=(X_valid_scaled, y_valid),\n",
    "#                     callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Técnicas de Regularização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Usando as Normas L1 e L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T15:18:46.978749Z",
     "start_time": "2021-10-06T15:18:46.911788Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for ℓ1 regularization with a factor of 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T15:22:23.973785Z",
     "start_time": "2021-10-06T15:22:23.643828Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Dropout\n",
    "- O neurônio tem probabilidade p de não ser usado no passo atual do treinamento: 20-30% nas redes recorrentes e 40-50% nas redes convolucionais.\n",
    "- Muitos modelos estado-da-arte só usam dropout na última camada escondida. Pode-se usar dropout apenas nas 3 últimas camadas escondidas se colocar dropout em todas levar a underfitting.\n",
    "- Para regularizar uma rede com ativação SELU e preservar a média e a variância das saídas, utilizar alpha dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T15:31:47.280935Z",
     "start_time": "2021-10-06T15:31:47.209976Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T15:49:33.998012Z",
     "start_time": "2021-10-06T15:47:50.422519Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6656 - accuracy: 0.7566 - val_loss: 0.6579 - val_accuracy: 0.8356\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5549 - accuracy: 0.7959 - val_loss: 0.5980 - val_accuracy: 0.8424\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5255 - accuracy: 0.8060 - val_loss: 0.5502 - val_accuracy: 0.8402\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5049 - accuracy: 0.8123 - val_loss: 0.4672 - val_accuracy: 0.8560\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4929 - accuracy: 0.8178 - val_loss: 0.4846 - val_accuracy: 0.8574\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4754 - accuracy: 0.8247 - val_loss: 0.4527 - val_accuracy: 0.8630\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4718 - accuracy: 0.8257 - val_loss: 0.4765 - val_accuracy: 0.8626\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4657 - accuracy: 0.8276 - val_loss: 0.4438 - val_accuracy: 0.8654\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4537 - accuracy: 0.8316 - val_loss: 0.3988 - val_accuracy: 0.8696\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4503 - accuracy: 0.8323 - val_loss: 0.4562 - val_accuracy: 0.8674\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4496 - accuracy: 0.8337 - val_loss: 0.4336 - val_accuracy: 0.8672\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4434 - accuracy: 0.8354 - val_loss: 0.4647 - val_accuracy: 0.8530\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4363 - accuracy: 0.8396 - val_loss: 0.3929 - val_accuracy: 0.8726\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4348 - accuracy: 0.8379 - val_loss: 0.4862 - val_accuracy: 0.8672\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4295 - accuracy: 0.8391 - val_loss: 0.4380 - val_accuracy: 0.8692\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4311 - accuracy: 0.8406 - val_loss: 0.4578 - val_accuracy: 0.8624\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4299 - accuracy: 0.8389 - val_loss: 0.4318 - val_accuracy: 0.8716\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4274 - accuracy: 0.8421 - val_loss: 0.4553 - val_accuracy: 0.8676\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4204 - accuracy: 0.8432 - val_loss: 0.4230 - val_accuracy: 0.8720\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4183 - accuracy: 0.8449 - val_loss: 0.4559 - val_accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Monte Carlo (MC) Dropout\n",
    "- Dá uma informação mais acurada sobre a incerteza da predição e também possui o potencial de melhorar a acurácia do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T15:58:58.149578Z",
     "start_time": "2021-10-06T15:58:26.915214Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)]) # [100, 10000, 10]\n",
    "y_proba = y_probas.mean(axis=0) # [10000, 10]\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T16:12:24.229249Z",
     "start_time": "2021-10-06T16:12:23.965109Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T16:12:53.598182Z",
     "start_time": "2021-10-06T16:12:53.576196Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.51, 0.  , 0.48]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.84, 0.  , 0.15]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.12, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.22, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.61, 0.  , 0.36]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.06, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.24, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.01, 0.  , 0.03, 0.23, 0.01, 0.11, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[0:10, :1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T16:13:43.784442Z",
     "start_time": "2021-10-06T16:13:43.776446Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.16, 0.  , 0.76]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T16:13:56.962880Z",
     "start_time": "2021-10-06T16:13:56.900915Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.01, 0.  , 0.15, 0.  , 0.21, 0.  , 0.27]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Observação\n",
    "- Caso haja outras camadas que tem comportamento diferente no treinamento e na predição, como Batch Normalization, é necessário alterar as camadas que tem dropout como está descrito abaixo.\n",
    "- O modelo já poderia ser construído diretamente dessa forma, mas, caso não, faz-se a alteração do modelo e copia os pesos da rede anterior para a nova rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T16:21:53.857949Z",
     "start_time": "2021-10-06T16:21:53.838960Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T16:22:51.340728Z",
     "start_time": "2021-10-06T16:22:51.210862Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T16:23:09.340028Z",
     "start_time": "2021-10-06T16:23:03.875005Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.15, 0.  , 0.77]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Regularização de Máxima Norma\n",
    "- O vetor de pesos é reescalonado conforme a máxima norma definida previamente: ou seja, esse critério não faz parte da função custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T15:04:08.525164Z",
     "start_time": "2021-10-07T15:04:08.510174Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal', \n",
    "                          kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
